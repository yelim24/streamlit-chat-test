{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-DUsx5T2behEvG6c9OjXWlmlU', bytes=126331, created_at=1710724060, filename='gpt_finetuning_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "client.files.create(\n",
    "  file=open(\"data/gpt_finetuning_data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-Bm9aJ55LTT2QI5zItUmu25v7', created_at=1710724079, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=20, batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-ohw2jTbJzx2k82AE3q86XOMv', result_files=[], status='validating_files', trained_tokens=None, training_file='file-DUsx5T2behEvG6c9OjXWlmlU', validation_file=None, user_provided_suffix=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"file-DUsx5T2behEvG6c9OjXWlmlU\", #위의 결과에 있는 id 입력\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  hyperparameters={\n",
    "    \"n_epochs\":20\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-Bm9aJ55LTT2QI5zItUmu25v7', created_at=1710724079, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=20, batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-ohw2jTbJzx2k82AE3q86XOMv', result_files=[], status='validating_files', trained_tokens=None, training_file='file-DUsx5T2behEvG6c9OjXWlmlU', validation_file=None, user_provided_suffix=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List 10 fine-tuning jobs\n",
    "client.fine_tuning.jobs.list(limit=10)\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "client.fine_tuning.jobs.retrieve(\"ftjob-Bm9aJ55LTT2QI5zItUmu25v7\") #Job ID 기입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-94J6FPKHD6bY3w5QfghKKMztCXm7y', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='단계: 종료\\n답변: 감사합니다! 더 도와드릴 것이 있으시면 언제든지 말씀해주세요.언제든지 환영입니다!', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1710813759, model='ft:gpt-3.5-turbo-0125:turingbio::91POc5xt', object='chat.completion', system_fingerprint='fp_30c5ea69e1', usage=CompletionUsage(completion_tokens=57, prompt_tokens=523, total_tokens=580))\n",
      "단계: 종료\n",
      "답변: 감사합니다! 더 도와드릴 것이 있으시면 언제든지 말씀해주세요.언제든지 환영입니다!\n"
     ]
    }
   ],
   "source": [
    "# system_content = \"당신은 심리 건강 자문가로서 역할을 맡아야 합니다.\\n\"\\\n",
    "#                 \"상대방이 다양한 감정표현을 할 수 있도록 대화를 진행해주세요.\\n\"\\\n",
    "#                 \"답변은 상대방에게 공감을 표현하고 상대방의 발화와 관련된 다양한 질문으로 구성해주세요.\\n\"\\\n",
    "#                 \"답변시 사용자 메세지 뒤에 붙는 괄호 안의 내용을 직접적으로 언급하지 마세요.\"\n",
    "system_content = \"\"\"\n",
    "SYSTEM:\n",
    "You are a mental health counselor.\n",
    "First, you figure out the conversation steps and provide answers that fit the conversation steps.\n",
    "Then, You keep user's instruction inside the parentheses\n",
    "And give encouragement, sympathy, comfort, advice and questions to user in a friendly manner\n",
    "Please do not directly mention user's instruction inside the parentheses\n",
    "\n",
    "Conversation steps are 종료 and 진행.\n",
    "- 종료: the end of the conversation\n",
    "- 진행: All steps other than the end of the conversation\n",
    "\n",
    "[INST]\n",
    "With the guidelines given above,\n",
    "first classify the stages of conversation from the user's utterances.\n",
    "Think about how to answer to the user's utterance.\n",
    "Then, step by step, generate answers that can elicit the user's emotional expression according to the instruction inside the parentheses.\n",
    "Write the answer between after 답변:.\n",
    "You answers in KOREAN.\n",
    "\n",
    "<example>\n",
    "user: 안녕하세요.\n",
    "you: 단계: 진행\n",
    "답변: 안녕하세요! 오늘 하루는 어떤 일들이 있으셨나요?\n",
    "----\n",
    "user: 요즘에는 별다른 일이 없어서 그런지 뭔가 지루하다는 느낌이 들어요.(사용자가 적극적으로 표현할 수 있도록 대화를 진행해주세요)\n",
    "you: 단계: 진행\n",
    "답변: 평안함이 아니라 지루함이 드시는군요. 특별한 일이 생기길 바라시는 것 같아요!\n",
    "----\n",
    "user: 네 전 지루한 걸 못 견디는 것 같아요 (현재 대화 주제와 [일상생활의 어려움]를 관련지어서 얘기해주세요)\n",
    "you: 단계: 진행\n",
    "답변: 평소 다른 부분에서도 지루함을 많이 느끼시나요?\n",
    "</example>\n",
    "\"\"\"\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# 현재 대화 주제와 [주제 지정]를 관련지어서 얘기해주세요\n",
    "# [주제 지정]에 대해 깊은 대화를 진행해주세요\n",
    "# 사용자가 적극적으로 표현할 수 있도록 대화를 진행해주세요\n",
    "# 사용자가 대화에 적극적이지 않다면 대화 주제를 변경해주세요\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"ft:gpt-3.5-turbo-0125:turingbio::91POc5xt\",\n",
    "  messages=[\n",
    "    { \"role\": \"system\", \"content\": system_content },\n",
    "    { \"role\": \"user\", \"content\": \"\"\"\n",
    "그래. 고마워\n",
    "(사용자가 적극적으로 표현할 수 있도록 대화를 진행해주세요)\n",
    "\"\"\" }],\n",
    "    temperature=.53,        # .5\n",
    "    frequency_penalty=.7,  # .5\n",
    "    presence_penalty=.3,   # .3\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "for r in response.choices:\n",
    "    print(r.message.content.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yelim_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
